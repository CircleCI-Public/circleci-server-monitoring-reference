# circleci-server-monitoring-reference
A reference for tools, configurations, and documentation used to monitor CircleCI server.

ðŸš§ **Under Development**

This repository is currently under active development and is not yet a supported resource. Please refer to it at your own discretion until further notice.

{{ template "chart.header" . }}
{{ template "chart.description" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

## Installing the Monitoring Stack

{{ template "chart.requirementsSection" . }}

### 1. Configure Server for the Monitoring Stack

To set up monitoring for a CircleCI server instance, you need to configure Telegraf to set up a Prometheus client and expose a metrics endpoint. Add the following configuration to the CircleCI **server** Helm chart values:

```yaml
telegraf:
  config:
    outputs:
      - file:
          files: ["stdout"]
      - prometheus_client:
          listen: ":9273"
```

### 2. Install Dependencies

Before installing the full chart, you must first install the dependency subcharts, including the Prometheus Custom Resource Definitions (CRDs) and the Grafana operator chart. This assumes you are installing it in the same namespace as your CircleCI server installation:

```bash
$ helm install {{ template "chart.name" . }} . --dependency-update --set global.enabled=false --set prometheusOperator.installCRDs=true -n <your-server-namespace>
```

> **_NOTE:_**  It's possible to install the monitoring stack in a different namespace than the CircleCI server installation. If you do so, set the `prometheus.serviceMonitor.selectorNamespaces` value with the target namespace.

### 3. Install the Helm Chart

Next, install the Helm chart using the following command:

```bash
$ helm upgrade --install {{ template "chart.name" . }} . --reset-values -n <your-server-namespace>
```

### 4. Verify Prometheus Is Up and Targeting Telegraf
To verify that Prometheus is working correctly and targeting Telegraf, use the following command to port-forward Prometheus:

```bash
$ kubectl port-forward svc/server-monitoring-prometheus 9090:9090 -n <your-namespace-here>
```

Then visit http://localhost:9090/targets in your browser. Verify that Telegraf appears as a target and that its state is "up".

![Prometheus UI showing Telegraf target as up](docs/images/prometheus-telegraf-targets.png)

### 5. Verify Grafana Is Up and Connected to Prometheus

To verify that Grafana is working correctly and connected to Prometheus, use the following command to port-forward Grafana:
```bash
$ kubectl port-forward svc/server-monitoring-grafana-service 3000:3000 <your-namespace-here>
```

Then visit http://localhost:3000 in your browser. Once logged in with the default credentials, navigate to http://localhost:3000/dashboards and verify that the default dashboards are present and populating with data.

![Prometheus UI showing Telegraf target as up](docs/images/grafana-dashboards.png)

### 6. Next Steps

After ensuring both Prometheus and Grafana are operational, consider these enhancements:

#### Security
Secure Grafana by configuring credentials:
```yaml
grafana:
  credentials:
    adminUser: "admin"
    adminPassword: "<your-secure-password-here>"
    existingSecretName: "<your-secret-here>"
```
> **_NOTE:_** Consider using Kubernetes secrets for dynamic credential management.

#### Expose Grafana Externally

For external access, modify the service type:
```yaml
grafanaoperator:
  grafana:
    service:
      type: LoadBalancer
```
> **_NOTE:_** Ensure to use secure ingress controllers and firewall rules.

#### Enabling Persistent Storage

Persist data by enabling storage for Prometheus and Grafana:
```yaml
prometheus:
  persistence:
    enabled: true
    storageClass: <your-custom-storage-class>
grafana:
  persistence:
    enabled: true
    storageClass: <your-custom-storage-class>
```
> **_NOTE:_** Use a custom storage class with a 'Retain' policy to secure data integrity.

{{ template "chart.valuesSection" . }}
